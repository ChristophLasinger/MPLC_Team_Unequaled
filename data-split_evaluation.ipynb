{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c718e56-7620-4bc0-9de8-8ee4fbbe03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d17c61-13b3-4b5f-85e1-c93268fedfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset has 8230 files\n",
      "We have 58 labels\n"
     ]
    }
   ],
   "source": [
    "# adjust your path\n",
    "DATA_SET_PATH = \"C:/Users/simon/Downloads/MLPC2025_classification\"\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(DATA_SET_PATH, 'metadata.csv'))\n",
    "files = metadata[\"filename\"]\n",
    "\n",
    "features_dir = 'audio_features'\n",
    "labels_dir = 'labels'\n",
    "categories = ['Airplane', 'Alarm', 'Beep/Bleep', 'Bell', 'Bicycle', 'Bird Chirp', 'Bus', 'Car', 'Cat Meow', 'Chainsaw', 'Clapping', 'Cough', 'Cow Moo', 'Cowbell', 'Crying', 'Dog Bark', 'Doorbell', 'Drip', 'Drums', 'Fire', 'Footsteps', 'Guitar', 'Hammer', 'Helicopter', 'Hiccup', 'Horn Honk', 'Horse Neigh', 'Insect Buzz', 'Jackhammer', 'Laughter', 'Lawn Mower', 'Motorcycle', 'Piano', 'Pig Oink', 'Power Drill', 'Power Saw', 'Rain', 'Rooster Crow', 'Saxophone', 'Sewing Machine', 'Sheep/Goat Bleat', 'Ship/Boat', 'Shout', 'Singing', 'Siren', 'Sneeze', 'Snoring', 'Speech', 'Stream/River', 'Thunder', 'Train', 'Truck', 'Trumpet', 'Vacuum Cleaner', 'Violin', 'Washing Machine', 'Waves', 'Wind']\n",
    "print(f\"Our dataset has {len(files)} files\")\n",
    "print(f\"We have {len(categories)} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a6f875-1979-4954-bd7f-f0f49443930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions similar to the tutorial\n",
    "# might adjust when annotators do not agree but there is not really a good way how to do this\n",
    "def aggregate_labels(file_labels):\n",
    "    __y = []\n",
    "    for frame_labels in file_labels:\n",
    "        if(sum(frame_labels) == 0):\n",
    "            __y.append([0])\n",
    "        elif(np.count_nonzero(frame_labels) == len(frame_labels)):\n",
    "             __y.append([1])\n",
    "        else: #The annotators don't agree on the label\n",
    "            __y.append([np.random.choice(frame_labels)])\n",
    "    return __y\n",
    "\n",
    "\n",
    "# this uses the whole embeddings (768 features) as X; might need to adjust this\n",
    "def read_files(file_names, num_to_read=1000):\n",
    "    X_train = []\n",
    "    Y_train = {}\n",
    "    for c in categories:\n",
    "        Y_train[c] = []\n",
    "    for f in file_names[:num_to_read]: #we are not loading the entire dataset due to processing time\n",
    "        if not os.path.exists(os.path.join(DATA_SET_PATH, features_dir , f.split('.')[0] + '.npz')):\n",
    "            continue\n",
    "        features = np.load(os.path.join(DATA_SET_PATH, features_dir , f.split('.')[0] + '.npz'))[\"embeddings\"]\n",
    "        X_train.append(features)\n",
    "        y = np.load(os.path.join(DATA_SET_PATH, labels_dir , f.split('.')[0] + '_labels.npz'))\n",
    "        for c in categories:\n",
    "            _y = aggregate_labels(y[c])\n",
    "            Y_train[c].extend(list(itertools.chain.from_iterable(_y)))\n",
    "    X_train = np.concatenate(X_train)\n",
    "    \n",
    "    return X_train, convert_y_dict_to_array(Y_train, categories)\n",
    "\n",
    "# Convert dictionary to numpy array (for all splits)\n",
    "def convert_y_dict_to_array(y_dict, categories):\n",
    "    y_array = np.zeros((len(y_dict[categories[0]]), len(categories)), dtype=int)\n",
    "    for i, c in enumerate(categories):\n",
    "        y_array[:, i] = y_dict[c]\n",
    "    return y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "625d08cd-804e-41d0-a256-69dfe6faddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we perform splits on the frame level consecutive (=highly correlated) frames would be distributed accross all splits\n",
    "# Therefore,\n",
    "# we perform splits on the file level to avoid data leakage\n",
    "\n",
    "nf = len(files)\n",
    "# I tried some random states and their class distributions; 0 produces very similar distributions for all splits\n",
    "sampled_files = files.sample(nf, random_state=0)\n",
    "\n",
    "# train set 70%, val set 20%, test set 10% --> could also val, test = 15 %\n",
    "train_files = sampled_files[:int(nf*0.7)]\n",
    "val_files = sampled_files[int(nf*0.7):int(nf*0.9)]\n",
    "test_files = sampled_files[int(nf*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c58df0-190b-444d-9588-570ad0d1d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_to_read is here the whole dataset, for testing something you can adjust\n",
    "# this takes a couple of minutes\n",
    "X_train, y_train = read_files(train_files, num_to_read=len(train_files))\n",
    "X_val, y_val = read_files(val_files, num_to_read=len(val_files))\n",
    "X_test, y_test = read_files(test_files, num_to_read=len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06d528d-1f98-416f-a269-de926ede8f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1076443, 768)\n",
      "y_train: (1076443, 58)\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes\n",
    "print(\"X_train:\", X_train.shape)  # (n_frames, 768) 768 features, currently the embeddings \n",
    "print(\"y_train:\", y_train.shape)  # (n_frames, 58) 58 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecb9c51-0743-4583-bce3-7a641e992fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform preprocessing on X\n",
    "# we only fit on train data and then transform all splits accodringly\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cc3b836-1a81-4604-bdc1-9948bc7f70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline from tutorial\n",
    "class Baseline_classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.majority_class = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        '''x_train is a numpy array of features with shape NxD, where N is the number of datapoints and D the feature dimension\n",
    "        y_train is a list of binary labels in the shape Nx1\n",
    "        '''\n",
    "        self.majority_class =  1 if sum(y_train) > len(y_train) / 2 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''x is a numpy array of features with shape NxD, where N is the number of datapoints and D the feature dimension\n",
    "        The function should return the predicted binary labels as a numpy array of shape Nx1\n",
    "        '''\n",
    "        predictions = np.zeros(x.shape[0]) + self.majority_class\n",
    "        return predictions\n",
    "\n",
    "# can predict all labels at once\n",
    "class MultiLabelBaseline():\n",
    "    def __init__(self):\n",
    "        self.majority_classes = None \n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''y_train shape: (n_samples, 58)'''\n",
    "        self.majority_classes = (np.mean(y_train, axis=0) > 0.5).astype(int)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Returns predictions of shape (n_samples, 58)'''\n",
    "        return np.tile(self.majority_classes, (X.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ce2a2c-b7d6-4d74-ae4d-e5d5678d30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I propose using F1-score since we have unbalanced class distributions\n",
    "# Assumes y_true and y_pred are binary arrays of shape (n_samples, n_classes); should be the case\n",
    "def get_f1_score(y_true, y_pred, weighted=False):\n",
    "    if weighted:\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted') # Weighted F1: weights each class by its frequency\n",
    "    else:\n",
    "        f1= f1_score(y_true, y_pred, average='macro') # counts each class equally important\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab3fed2c-f4d4-471b-a6f9-32eac310dc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1-score 0.483447061072054\n",
      "Val f1-score 0.48218724109362054\n",
      "Accuracy train 0.93591\n",
      "Accuracy val 0.9577\n"
     ]
    }
   ],
   "source": [
    "# example of eval for a single class\n",
    "wind_x_train, wind_y_train = X_train_scaled[:100000], y_train[:100000, categories.index(\"Wind\")] # use only 100000 frames\n",
    "wind_x_val, wind_y_val = X_val_scaled[:10000], y_val[:10000, categories.index(\"Wind\")] # use only 10000 frames\n",
    "\n",
    "baseline = Baseline_classifier()\n",
    "baseline.fit(wind_x_train, wind_y_train)\n",
    "\n",
    "y_train_pred_wind = baseline.predict(wind_x_train)\n",
    "y_val_pred_wind = baseline.predict(wind_y_val)\n",
    "\n",
    "f1_train = get_f1_score(wind_y_train, y_train_pred_wind)\n",
    "f1_val = get_f1_score(wind_y_val, y_val_pred_wind)\n",
    "\n",
    "acc_train = accuracy_score(wind_y_train, y_train_pred_wind)# accuracy here just for \n",
    "acc_val = accuracy_score(wind_y_val, y_val_pred_wind)\n",
    "\n",
    "print(f\"Train f1-score {f1_train}\")\n",
    "print(f\"Val f1-score {f1_test}\")\n",
    "print(f\"Accuracy train {acc_train}\")\n",
    "print(f\"Accuracy val {acc_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b767e73-75ee-472d-8b92-b79a51a13297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_pred_wind)) # predicts only 0\n",
    "print(np.unique(y_test_pred_wind)) # as a matter of fact the baseline will always predict 0 since there is no class that occurres in >50% of frames, so one could also set the baseline to just 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dd405d8-843a-48c7-80cf-3749d8c50d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f1-score: 0.6921392642364108\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# example of eval for multiple classes\n",
    "x_train, y_train = X_train_scaled[:100000], y_train[:100000]\n",
    "x_val, y_val = X_val_scaled[:10000], y_val[:10000]\n",
    "\n",
    "multi_baseline = MultiLabelBaseline()\n",
    "multi_baseline.fit(x_train, y_train)\n",
    "\n",
    "y_val_pred = multi_baseline.predict(y_val)\n",
    "\n",
    "f1_scores = []\n",
    "for label in range(len(categories)):\n",
    "    f1_scores.append(get_f1_score(y_val[:, label], y_val_pred[:, label]))\n",
    "\n",
    "f1_score_mean = np.mean(f1_scores)\n",
    "\n",
    "print(f\"Mean f1-score: {f1_score_mean}\")\n",
    "print(np.unique(y_val_pred)) # all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8b3a4-8129-4967-875a-ffc189304683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
